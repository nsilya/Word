# Обо мне

Меня зовут Илья Нашахалов, я инженер данных.

Я специализируюсь на проектировании и разработке конвейеров обработки данных — от их сбора и трансформации до хранения и подготовки к аналитике. Мой практический опыт включает работу с различными СУБД (MongoDB, PostgreSQL, ClickHouse), брокерами сообщений (Kafka), инструментами оркестрации (Airflow, Spark Streaming) и системами визуализации (Grafana).

## Ключевые проекты

Ниже приведены ключевые проекты, которые я реализовал:

1.  **Аналитическая система "Пикча" (ETL/ELT Pipeline)**: Разработал комплексную систему, имитирующую аналитику для сети магазинов. Она включала генерацию данных, их загрузку в MongoDB, передачу через Kafka, шифрование персональных данных (email, телефон), загрузку в слои RAW и MART в ClickHouse и визуализацию в Grafana. Также реализовал алертинг через Telegram-бота.
2.  **Автоматизация с Apache Airflow (`airflow_dag_generator`)**: Создал систему оркестрации ETL-процесса проекта "Пикча" с помощью Apache Airflow, что повысило надежность и управляемость.
3.  **Аналитика с Trino**: Интегрировал Trino для выполнения сложных аналитических SQL-запросов к данным в ClickHouse, расширяя возможности ad-hoc анализа.
4.  **Захват изменений (CDC) с Debezium**: Реализовал отдельный проект для работы с потоками данных в реальном времени. Использовал Debezium для отслеживания изменений в PostgreSQL и передачи их в Kafka, откуда они_consumировались и загружались в ClickHouse.
5.  **Потоковая обработка с Spark (`Streaming_Spark`)**: Разработал демонстрационную систему потоковой аналитики с использованием PySpark Structured Streaming и Kafka. Она включала агрегацию данных по временным окнам с учетом event-time и watermark'ов.

Эти проекты позволили мне получить глубокие практические навыки в области Data Engineering: построение пайплайнов, работа с потоковыми данными, автоматизация процессов, обеспечение качества и безопасности данных. Я свободно владею Python, SQL, Docker и перечисленными выше инструментами.
